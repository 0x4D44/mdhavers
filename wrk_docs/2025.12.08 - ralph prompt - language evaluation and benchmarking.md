# Ralph Wiggum Prompt: mdhavers Language Evaluation and Benchmarking

## Objective

Comprehensively evaluate the mdhavers programming language against real-world scenarios, testing resilience, performance, and scalability. Compare results with equivalent Rust implementations to establish baseline performance expectations.

## Evaluation Dimensions

### 1. Performance Benchmarking

Create and run benchmarks comparing mdhavers (both JS and LLVM backends) against Rust:

#### Computational Benchmarks
| Benchmark | Description | Metrics |
|-----------|-------------|---------|
| `fib_recursive(40)` | Naive recursive fibonacci | Time, stack depth |
| `fib_iterative(1M)` | Iterative fibonacci to 1M iterations | Time, memory |
| `primes_sieve(100K)` | Sieve of Eratosthenes to 100K | Time, memory |
| `matrix_mult(500x500)` | Matrix multiplication | Time, cache efficiency |
| `string_concat(100K)` | String concatenation 100K times | Time, memory, GC pressure |
| `list_ops(1M)` | 1M list operations (push/pop/access) | Time, memory |
| `sort_large(100K)` | Sort 100K element list | Time, comparison count |
| `json_parse_large` | Parse large JSON-like structure | Time, memory |

#### Create benchmark files:
```
benchmarks/stress/
├── computational/
│   ├── fib_stress.braw
│   ├── primes_stress.braw
│   ├── matrix_mult.braw
│   └── string_stress.braw
├── memory/
│   ├── list_stress.braw
│   ├── dict_stress.braw
│   └── allocation_stress.braw
└── rust/
    ├── fib_stress.rs
    ├── primes_stress.rs
    └── ... (equivalent Rust implementations)
```

### 2. Scalability Testing

Test how mdhavers handles increasing workloads:

#### Data Size Scaling
- Process lists of 1K, 10K, 100K, 1M elements
- Measure time complexity (should be O(n) for linear ops)
- Identify any unexpected O(n²) or worse behaviors

#### Recursion Depth
- Test maximum recursion depth before stack overflow
- Compare LLVM vs JS backend stack limits
- Document any tail-call optimization effects

#### Memory Scaling
- Track memory usage as data grows
- Identify memory leaks in long-running operations
- Test garbage collection behavior (JS backend)

### 3. Resilience Testing

Test error handling and edge cases:

#### Error Scenarios
| Test Case | Expected Behavior |
|-----------|-------------------|
| Division by zero | Graceful error message |
| Stack overflow | Caught and reported |
| Out of memory | Graceful degradation |
| Invalid type operations | Clear error messages |
| Empty list operations | Proper nil/error handling |
| Unicode strings | Correct handling |
| Very large numbers | Integer overflow handling |
| Deeply nested structures | Memory/recursion limits |

#### Create edge case tests:
```
benchmarks/edge_cases/
├── error_handling.braw
├── type_coercion.braw
├── boundary_values.braw
├── unicode_strings.braw
└── nested_structures.braw
```

### 4. Real-World Application Tests

Implement small but realistic applications:

#### Applications to Build
1. **CSV Parser** - Parse and process a 10K row CSV file
2. **Simple HTTP-like Parser** - Parse request/response structures
3. **Expression Evaluator** - Math expression parser and evaluator
4. **Game of Life** - Conway's Game of Life simulation
5. **Maze Solver** - BFS/DFS pathfinding algorithm

```
benchmarks/applications/
├── csv_parser.braw
├── expr_evaluator.braw
├── game_of_life.braw
└── maze_solver.braw
```

### 5. Comparison Framework

Create a unified benchmarking harness:

```bash
#!/bin/bash
# benchmarks/run_comparison.sh

run_benchmark() {
    local name=$1
    local iterations=${2:-5}

    echo "=== $name ==="

    # mdhavers JS backend
    echo "JS Backend:"
    time for i in $(seq 1 $iterations); do
        node "$name.js" > /dev/null
    done

    # mdhavers LLVM backend
    echo "LLVM Backend:"
    ./target/release/mdhavers build "$name.braw" -o "/tmp/$name"
    time for i in $(seq 1 $iterations); do
        "/tmp/$name" > /dev/null
    done

    # Rust equivalent
    if [ -f "rust/$name" ]; then
        echo "Rust:"
        time for i in $(seq 1 $iterations); do
            "rust/$name" > /dev/null
        done
    fi
}
```

## Metrics to Collect

For each benchmark, record:

1. **Execution Time**
   - Wall clock time
   - User CPU time
   - System CPU time

2. **Memory Usage**
   - Peak memory (RSS)
   - Heap allocations
   - Final memory after cleanup

3. **Correctness**
   - Output matches expected
   - No crashes or hangs
   - Error messages are helpful

4. **Scalability Factor**
   - Time(2N) / Time(N) ratio
   - Memory(2N) / Memory(N) ratio

## Output Format

Generate a results report:

```markdown
# mdhavers Performance Evaluation Report

## Executive Summary
- Overall performance vs Rust: X.Xx slower
- LLVM vs JS speedup: X.Xx faster
- Memory efficiency: X% of Rust baseline

## Detailed Results

### Computational Benchmarks
| Benchmark | JS (ms) | LLVM (ms) | Rust (ms) | LLVM/Rust |
|-----------|---------|-----------|-----------|-----------|
| fib(40)   | ...     | ...       | ...       | ...       |

### Scalability Analysis
[Charts/tables showing O(n) behavior]

### Resilience Summary
[Pass/fail for each edge case]

### Recommendations
[Areas for improvement]
```

## Success Criteria

1. **All benchmarks run without crashes**
2. **Results documented in `benchmarks/results/evaluation_report.md`**
3. **LLVM backend within 5x of Rust for computational tasks**
4. **All edge cases handled gracefully**
5. **At least 3 real-world applications working correctly**

## Execution Plan

1. **Phase 1**: Create stress test benchmarks (computational)
2. **Phase 2**: Create equivalent Rust implementations
3. **Phase 3**: Run comparisons, collect metrics
4. **Phase 4**: Create edge case tests, run resilience tests
5. **Phase 5**: Build real-world applications
6. **Phase 6**: Generate final report

## Commands

Build and test:
```bash
cargo build --release --features llvm
./benchmarks/run_comparison.sh
```

## Completion Promise

Output `<promise>evaluation-complete</promise>` when:
1. All benchmarks created and running
2. Rust comparisons completed
3. Results report generated
4. All edge cases tested
