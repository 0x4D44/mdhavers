# mdhavers Stress Testing and Benchmarks - Detailed Implementation Prompt

## Goal
Stress test the mdhavers language with larger, more complex projects to explore resilience, performance, and scalability. Compare performance against equivalent Rust implementations to identify optimization opportunities.

## Success Criteria
1. Create a comprehensive benchmark suite in `benchmarks/` directory
2. Implement at least 5 significant algorithms/programs in mdhavers
3. Create equivalent Rust implementations for comparison
4. Generate benchmark report with timing comparisons
5. Identify and document any bugs, limitations, or edge cases found
6. All benchmark programs run successfully without crashes

## Benchmark Categories

### 1. Recursive Algorithms
Test stack depth and recursive performance:
- **Fibonacci** - naive recursive (stress test), memoized, iterative
- **Factorial** - recursive with large numbers
- **Ackermann function** - extreme recursion stress test
- **Tree traversal** - binary tree operations

### 2. Sorting Algorithms
Test list manipulation and comparison operations:
- **Bubble sort** - O(n²) baseline
- **Quick sort** - recursive partitioning
- **Merge sort** - divide and conquer with list concatenation
- **Insertion sort** - in-place modifications

### 3. Mathematical Computations
Test arithmetic operations at scale:
- **Prime sieve** - Sieve of Eratosthenes
- **Prime factorization** - factor large numbers
- **Greatest common divisor** - Euclidean algorithm
- **Monte Carlo Pi estimation** - random number generation (if available)

### 4. String Processing
Test string operations:
- **Palindrome checker** - string reversal and comparison
- **Word frequency counter** - parsing and counting
- **Simple pattern matching** - basic search operations
- **String concatenation stress** - build large strings

### 5. Data Structure Operations
Test list operations at scale:
- **List reversal** - various sizes
- **List filtering** - conditional operations
- **Nested list operations** - deep nesting
- **Map/reduce patterns** - functional operations

### 6. Real-World Programs
Test complete program functionality:
- **Calculator** - expression parser and evaluator
- **Game of Life** - cellular automaton (if I/O permits)
- **Maze solver** - pathfinding algorithm
- **Simple interpreter** - meta-circular evaluation

## Directory Structure

```
benchmarks/
├── README.md                    # Benchmark documentation
├── run_benchmarks.sh            # Script to run all benchmarks
├── results/                     # Benchmark results
│   └── report.md               # Generated comparison report
├── mdhavers/                    # mdhavers implementations
│   ├── fibonacci.braw
│   ├── factorial.braw
│   ├── quicksort.braw
│   ├── mergesort.braw
│   ├── primes.braw
│   ├── gcd.braw
│   ├── palindrome.braw
│   ├── list_ops.braw
│   └── stress_test.braw        # Combined stress test
├── rust/                        # Equivalent Rust implementations
│   ├── Cargo.toml
│   └── src/
│       ├── main.rs
│       ├── fibonacci.rs
│       ├── factorial.rs
│       ├── quicksort.rs
│       ├── mergesort.rs
│       ├── primes.rs
│       └── gcd.rs
└── edge_cases/                  # Edge case tests
    ├── deep_recursion.braw
    ├── large_lists.braw
    ├── many_variables.braw
    └── complex_expressions.braw
```

## Benchmark Implementations

### Fibonacci (fibonacci.braw)
```
dae fib_naive(n) {
    gin n <= 1 { gie n }
    gie fib_naive(n - 1) + fib_naive(n - 2)
}

dae fib_iter(n) {
    gin n <= 1 { gie n }
    ken a = 0
    ken b = 1
    ken i = 2
    whiles i <= n {
        ken temp = a + b
        a = b
        b = temp
        i = i + 1
    }
    gie b
}
```

### Quick Sort (quicksort.braw)
```
dae quicksort(arr) {
    gin len(arr) <= 1 { gie arr }
    ken pivot = arr[0]
    ken less = []
    ken greater = []
    ken i = 1
    whiles i < len(arr) {
        gin arr[i] < pivot {
            push(less, arr[i])
        } ither {
            push(greater, arr[i])
        }
        i = i + 1
    }
    gie quicksort(less) + [pivot] + quicksort(greater)
}
```

### Prime Sieve (primes.braw)
```
dae sieve(n) {
    ken is_prime = []
    ken i = 0
    whiles i <= n {
        push(is_prime, aye)
        i = i + 1
    }
    is_prime[0] = nae
    is_prime[1] = nae

    ken p = 2
    whiles p * p <= n {
        gin is_prime[p] {
            ken j = p * p
            whiles j <= n {
                is_prime[j] = nae
                j = j + p
            }
        }
        p = p + 1
    }

    ken primes = []
    ken k = 2
    whiles k <= n {
        gin is_prime[k] {
            push(primes, k)
        }
        k = k + 1
    }
    gie primes
}
```

## Rust Equivalent Implementations

For fair comparison, create equivalent Rust programs that:
- Use standard library only (no external crates)
- Mirror the algorithm structure of mdhavers versions
- Measure execution time with `std::time::Instant`
- Print results in parseable format

### Example Rust fibonacci.rs
```rust
use std::time::Instant;

fn fib_naive(n: u64) -> u64 {
    if n <= 1 { return n; }
    fib_naive(n - 1) + fib_naive(n - 2)
}

fn fib_iter(n: u64) -> u64 {
    if n <= 1 { return n; }
    let (mut a, mut b) = (0, 1);
    for _ in 2..=n {
        let temp = a + b;
        a = b;
        b = temp;
    }
    b
}

fn main() {
    let n = 35;

    let start = Instant::now();
    let result = fib_naive(n);
    let naive_time = start.elapsed();
    println!("fib_naive({}) = {} in {:?}", n, result, naive_time);

    let start = Instant::now();
    let result = fib_iter(n);
    let iter_time = start.elapsed();
    println!("fib_iter({}) = {} in {:?}", n, result, iter_time);
}
```

## Benchmark Script (run_benchmarks.sh)

The script should:
1. Build Rust benchmarks in release mode
2. Run each mdhavers benchmark with timing
3. Run each Rust benchmark
4. Collect results into a markdown report
5. Calculate performance ratios

```bash
#!/bin/bash
# Run benchmarks and generate comparison report

RESULTS_DIR="benchmarks/results"
mkdir -p "$RESULTS_DIR"

echo "# Benchmark Results" > "$RESULTS_DIR/report.md"
echo "Generated: $(date)" >> "$RESULTS_DIR/report.md"
echo "" >> "$RESULTS_DIR/report.md"

# Benchmark configurations
declare -A TESTS=(
    ["fibonacci"]="35"
    ["factorial"]="20"
    ["primes"]="10000"
    ["quicksort"]="1000"
)

for test in "${!TESTS[@]}"; do
    param="${TESTS[$test]}"
    echo "## $test (n=$param)" >> "$RESULTS_DIR/report.md"

    # Run mdhavers
    mdhavers_time=$( { time mdhavers run "benchmarks/mdhavers/${test}.braw" ; } 2>&1 )

    # Run Rust
    rust_time=$( { time ./benchmarks/rust/target/release/benchmark_${test} ; } 2>&1 )

    echo "| Language | Time |" >> "$RESULTS_DIR/report.md"
    echo "|----------|------|" >> "$RESULTS_DIR/report.md"
    echo "| mdhavers | $mdhavers_time |" >> "$RESULTS_DIR/report.md"
    echo "| Rust | $rust_time |" >> "$RESULTS_DIR/report.md"
done
```

## Edge Cases to Test

### Deep Recursion (deep_recursion.braw)
- Test maximum stack depth
- Identify stack overflow behavior
- Test tail recursion (if optimized)

### Large Lists (large_lists.braw)
- Create lists with 10,000+ elements
- Test memory behavior
- Test iteration performance

### Many Variables (many_variables.braw)
- Declare 1000+ variables
- Test scope handling
- Test variable lookup performance

### Complex Expressions (complex_expressions.braw)
- Deeply nested expressions
- Long operator chains
- Complex precedence patterns

## Metrics to Collect

1. **Execution Time** - Wall clock time for each benchmark
2. **Memory Usage** - Peak memory consumption (if measurable)
3. **Correctness** - Verify results match expected output
4. **Stability** - No crashes or unexpected behavior
5. **Error Messages** - Quality of error messages on failure

## Expected Findings

Document discoveries in these areas:
- Performance bottlenecks in interpreter
- LLVM compilation speedups vs interpreter
- Memory usage patterns
- Stack limitations
- Parser/lexer edge cases
- Missing language features needed for benchmarks

## Report Format

The final report should include:
1. Summary table of all benchmarks
2. Performance ratio (Rust time / mdhavers time)
3. Graphs if possible (or ASCII charts)
4. List of bugs/issues discovered
5. Recommendations for optimization
6. Language feature requests identified

## Implementation Order

1. **Phase 1: Setup**
   - Create benchmark directory structure
   - Create benchmark runner script
   - Set up Rust benchmark project

2. **Phase 2: Core Benchmarks**
   - Implement fibonacci (both versions)
   - Implement factorial
   - Implement prime sieve
   - Implement sorting algorithms

3. **Phase 3: Rust Equivalents**
   - Port all algorithms to Rust
   - Verify correctness
   - Add timing measurements

4. **Phase 4: Edge Cases**
   - Create stress test programs
   - Test limits of language
   - Document any crashes/errors

5. **Phase 5: Analysis**
   - Run all benchmarks
   - Generate comparison report
   - Document findings

## Notes

- Run interpreter benchmarks AND native compiled benchmarks (if LLVM available)
- Compare interpreter vs native compilation performance
- Test both small inputs (correctness) and large inputs (performance)
- Document any workarounds needed for missing features
- Note any language changes that would improve benchmark code
